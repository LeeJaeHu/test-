{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차제곱합\n",
    "def square_sum(y,t):\n",
    "    return 1.0/2.0 * np.sum(np.square(y-t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차엔트로피(CEE)\n",
    "# 분류 문제에서 원핫인코딩(정답만 1 나머지 0) 이라서\n",
    "def cross_entropy(y, t):\n",
    "    return -np.sum(t * np.log(y+1e-7)) # y = 0 이면 무한대가 되는걸 방지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient decent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent - 기울기 수정마다 데이터 무작위 선출\n",
    "# Momentum - 관성적용, 적합할 모수가 하나 더 늘어남 \n",
    "# AdaGrad - 학습률이 점점 작아짐, 도중에 0이 되어 더이상 학습이 안 될 수 있음\n",
    "# RMSProp - 학습률에 관여하는 h를 이전 h를 적당한 비율로 포함하여 적당히 줄어들게함\n",
    "# Adam - momentum + adagrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치학습 - 배치사이즈가 훈련데이터 수, 국소 최적해에 빠지기 쉬움\n",
    "# 온라인 학습 - 배치사이즈가 1, 국소에는 안 빠지지만 안정성이 떨어짐\n",
    "# 미니배치 - 무작위로 배치사이즈만큼 데이터 샘플 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 회귀에서 구현\n",
    "class Outputlayer:\n",
    "    def __init__(self, n_upper, n):\n",
    "        self.w = wb_width * np.random.randn(n_upper, n)\n",
    "        self.b = wb_width * np.random.randn(n)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        u = np.dot(x, self.w) + self.b\n",
    "        self.y = u\n",
    "        \n",
    "    def backward(self, t):\n",
    "        delta = self.y - t\n",
    "        \n",
    "        self.grad_w = np.dot(self.x.T, delta)\n",
    "        self.grad_b = np.sum(delta, axis=0)\n",
    "        \n",
    "        self.grad_x = np.dot(delta, self.w.T)\n",
    "        \n",
    "    def update(self, eta):\n",
    "        self.w -= eta * self.grad_w\n",
    "        self.b -= eta * self.grad_b\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
