{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPKvKMkAkRMn"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "TWofNaR-kS1s"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xWVObL142EBs"
   },
   "source": [
    "# Writing custom layers and models with Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7VqaVrAvw9j"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/custom_layers_and_models\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/keras/custom_layers_and_models.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/keras/custom_layers_and_models.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/keras/custom_layers_and_models.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NrUIvL8oxlhj"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Szd0mNROxqJ7"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVsTiIb62IbJ"
   },
   "source": [
    "## The Layer class\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0KRUQWG2k4v"
   },
   "source": [
    "### Layers encapsulate a state (weights) and some computation\n",
    "\n",
    "The main data structure you'll work with is the `Layer`.\n",
    "A layer encapsulates both a state (the layer's \"weights\")\n",
    "and a transformation from inputs to outputs (a \"call\", the layer's\n",
    "forward pass).\n",
    "\n",
    "Here's a densely-connected layer. It has a state: the variables `w` and `b`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LisHKABR2-Nj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.02889749 -0.10398108 -0.00575986  0.01895376]\n",
      " [-0.02889749 -0.10398108 -0.00575986  0.01895376]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n",
    "                                                  dtype='float32'),\n",
    "                             trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,),\n",
    "                                                  dtype='float32'),\n",
    "                             trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[ 0.09302854,  0.007962  ,  0.01599212,  0.05290163],\n",
       "        [-0.05164344,  0.03467279, -0.02845521,  0.02873094]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[ 0.09302854,  0.007962  ,  0.01599212,  0.05290163],\n",
       "        [-0.05164344,  0.03467279, -0.02845521,  0.02873094]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[ 0.02054681, -0.03626327, -0.0118773 , -0.02946744],\n",
       "        [-0.0011623 , -0.04729664,  0.06191743, -0.0300195 ]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y8RsI6Hr2OOd"
   },
   "source": [
    "Note that the weights `w` and `b` are automatically tracked by the layer upon\n",
    "being set as layer attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7x3Hl8m2XEJ"
   },
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXQPwqEs2gCH"
   },
   "source": [
    "Note you also have access to a quicker shortcut for adding weight to a layer: the `add_weight` method:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8LSx6HDg2iPz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04952537  0.10652478  0.01390582 -0.07114404]\n",
      " [-0.04952537  0.10652478  0.01390582 -0.07114404]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                 initializer='zeros',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[-0.03324789, -0.04835039,  0.01064808,  0.00119833],\n",
       "        [-0.01639398, -0.03614883, -0.00842738,  0.04886182]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vXjjEthGgr4y"
   },
   "source": [
    "#### Layers can have non-trainable weights\n",
    "\n",
    "Besides trainable weights, you can add non-trainable weights to a layer as well.\n",
    "Such weights are meant not to be taken into account during backpropagation,\n",
    "when you are training the layer.\n",
    "\n",
    "Here's how to add and use a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIIfmpDIgyUy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(layers.Layer):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
    "                                 trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4. 4.]\n",
      "[8. 8.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(layers.Layer):\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
    "                                trainable = False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "x = tf.ones((4, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qLiWVq-3g0c0"
   },
   "source": [
    "It's part of `layer.weights`, but it gets categorized as a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X7RhEZNvg2dE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "print('weights:', len(my_sum.weights))\n",
    "print('non-trainable weights:', len(my_sum.non_trainable_weights))\n",
    "\n",
    "# It's not included in the trainable weights:\n",
    "print('trainable_weights:', my_sum.trainable_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DOwYZ-Ew329E"
   },
   "source": [
    "### Best practice: deferring weight creation until the shape of the inputs is known\n",
    "\n",
    "In the logistic regression example above, our `Linear` layer took an `input_dim` argument\n",
    "that was used to compute the shape of the weights `w` and `b` in `__init__`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tzxUxoPc3Esh"
   },
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape=(input_dim, units),\n",
    "                                initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        self.b = self.add_weight(shape=(units,),\n",
    "                                initializer='zeros',\n",
    "                                trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ejSYZGaP4CD6"
   },
   "source": [
    "In many cases, you may not know in advance the size of your inputs, and you would\n",
    "like to lazily create weights when that value becomes known,\n",
    "some time after instantiating the layer.\n",
    "\n",
    "In the Keras API, we recommend creating layer weights in the `build(inputs_shape)` method of your layer.\n",
    "Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AGhRg7Nt4EB8"
   },
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_a(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear_a, self).__init__()\n",
    "        self.units = units\n",
    "        print(\"init\")\n",
    "\n",
    "    def build(self, a):\n",
    "        self.w = self.add_weight(shape=(a[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        print(\"build\")\n",
    "\n",
    "    def call(self, abc):\n",
    "        print(\"call\")\n",
    "        return tf.matmul(abc, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "build\n",
      "call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=475, shape=(2, 32), dtype=float32, numpy=\n",
       "array([[-0.05962064,  0.05254285,  0.04065034, -0.02519245, -0.00860368,\n",
       "         0.00766941, -0.0459495 , -0.01660534, -0.16014819, -0.16301665,\n",
       "        -0.034472  ,  0.18118721, -0.0761043 , -0.09365515,  0.04958824,\n",
       "         0.01287301,  0.04132933, -0.06629962, -0.02524884,  0.176397  ,\n",
       "         0.14697832,  0.10170928,  0.0081564 ,  0.00733461,  0.06406184,\n",
       "         0.00845177,  0.08103774, -0.04317323,  0.04183863,  0.0579142 ,\n",
       "        -0.04370681, -0.166964  ],\n",
       "       [-0.05962064,  0.05254285,  0.04065034, -0.02519245, -0.00860368,\n",
       "         0.00766941, -0.0459495 , -0.01660534, -0.16014819, -0.16301665,\n",
       "        -0.034472  ,  0.18118721, -0.0761043 , -0.09365515,  0.04958824,\n",
       "         0.01287301,  0.04132933, -0.06629962, -0.02524884,  0.176397  ,\n",
       "         0.14697832,  0.10170928,  0.0081564 ,  0.00733461,  0.06406184,\n",
       "         0.00845177,  0.08103774, -0.04317323,  0.04183863,  0.0579142 ,\n",
       "        -0.04370681, -0.166964  ]], dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer_a = Linear_a(32)\n",
    "linear_layer_a(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'linear_a_6/Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
       "array([[-0.10247622,  0.05028255, -0.01717131, -0.00418463,  0.03603247,\n",
       "         0.02031094,  0.075741  , -0.02484997,  0.01153527, -0.03117465,\n",
       "        -0.0557558 ,  0.06679498,  0.01571436, -0.03417717, -0.04535168,\n",
       "         0.05258175,  0.02755596,  0.06353343,  0.04188916, -0.00650458,\n",
       "         0.06517434,  0.01466364, -0.00290745, -0.04499962, -0.0052455 ,\n",
       "        -0.0838006 ,  0.05636636, -0.00766802, -0.00542857, -0.00263645,\n",
       "         0.00603967, -0.03574076],\n",
       "       [ 0.02222881,  0.0104343 ,  0.09268503,  0.01864413,  0.02977577,\n",
       "         0.00432332, -0.0498291 ,  0.03441909, -0.08387384, -0.04579395,\n",
       "         0.00448197,  0.04648512, -0.02968965, -0.05317941,  0.12420692,\n",
       "         0.0055695 , -0.02151216, -0.01883881, -0.06344761,  0.04110399,\n",
       "         0.01639282,  0.02360121,  0.02974799,  0.05391997,  0.06019765,\n",
       "         0.0913583 , -0.06671597,  0.00603999,  0.07442065,  0.00870804,\n",
       "        -0.05419933, -0.04800944]], dtype=float32)>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer_a.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'linear_a_6/Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
       " array([[-0.10247622,  0.05028255, -0.01717131, -0.00418463,  0.03603247,\n",
       "          0.02031094,  0.075741  , -0.02484997,  0.01153527, -0.03117465,\n",
       "         -0.0557558 ,  0.06679498,  0.01571436, -0.03417717, -0.04535168,\n",
       "          0.05258175,  0.02755596,  0.06353343,  0.04188916, -0.00650458,\n",
       "          0.06517434,  0.01466364, -0.00290745, -0.04499962, -0.0052455 ,\n",
       "         -0.0838006 ,  0.05636636, -0.00766802, -0.00542857, -0.00263645,\n",
       "          0.00603967, -0.03574076],\n",
       "        [ 0.02222881,  0.0104343 ,  0.09268503,  0.01864413,  0.02977577,\n",
       "          0.00432332, -0.0498291 ,  0.03441909, -0.08387384, -0.04579395,\n",
       "          0.00448197,  0.04648512, -0.02968965, -0.05317941,  0.12420692,\n",
       "          0.0055695 , -0.02151216, -0.01883881, -0.06344761,  0.04110399,\n",
       "          0.01639282,  0.02360121,  0.02974799,  0.05391997,  0.06019765,\n",
       "          0.0913583 , -0.06671597,  0.00603999,  0.07442065,  0.00870804,\n",
       "         -0.05419933, -0.04800944]], dtype=float32)>,\n",
       " <tf.Variable 'linear_a_6/Variable:0' shape=(32,) dtype=float32, numpy=\n",
       " array([ 0.02062676, -0.00817399, -0.03486338, -0.03965195, -0.07441191,\n",
       "        -0.01696485, -0.0718614 , -0.02617446, -0.08780961, -0.08604804,\n",
       "         0.01680183,  0.06790711, -0.06212901, -0.00629857, -0.02926701,\n",
       "        -0.04527824,  0.03528553, -0.11099423, -0.00369039,  0.14179759,\n",
       "         0.06541116,  0.06344443, -0.01868413, -0.00158573,  0.00910969,\n",
       "         0.00089406,  0.09138735, -0.0415452 , -0.02715345,  0.05184262,\n",
       "         0.00445286, -0.08321381], dtype=float32)>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer_a.trainable_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear3(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear3, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear5(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear5, self).__init__()\n",
    "        self.units = units\n",
    "        print(\"init\")\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print('build')\n",
    "    def call(self, inputs):\n",
    "        print(\"call\")\n",
    "        return tf.matmul(inputs, self.weights[0][0]) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "build\n",
      "call\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-4ff24830a584>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlinear_layer5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinear5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlinear_layer5\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-65-c3bba3bcbbb3>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"call\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "linear_layer5 = Linear5(32)\n",
    "\n",
    "linear_layer5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer5.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'linear5_40/Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
       " array([[-0.07900801, -0.01455089, -0.04886997,  0.00849197,  0.04231484,\n",
       "          0.00241421, -0.06481494, -0.00893692, -0.01957316, -0.00694142,\n",
       "          0.04600096,  0.01236336,  0.08572765, -0.0362857 ,  0.00192356,\n",
       "         -0.04226952, -0.05431823, -0.0406916 ,  0.01047941,  0.05529768,\n",
       "          0.05197164, -0.10651936, -0.05965484, -0.06841718, -0.03939276,\n",
       "          0.04163815,  0.01368979, -0.06939536, -0.01541925, -0.05483044,\n",
       "          0.04326602, -0.01088315],\n",
       "        [-0.06114447,  0.01810847,  0.05237279, -0.0096415 ,  0.04506547,\n",
       "          0.05059414,  0.05961224,  0.01779534, -0.06088741, -0.03586103,\n",
       "          0.0339044 ,  0.06761163, -0.01116273, -0.05239036,  0.02479657,\n",
       "          0.0050546 , -0.02378914, -0.07779925, -0.02153455,  0.06172351,\n",
       "         -0.15516023,  0.01414393,  0.12694876, -0.03168827,  0.0452406 ,\n",
       "          0.00567624, -0.03010116,  0.04501907,  0.00605122,  0.00128576,\n",
       "         -0.02311309,  0.01309393]], dtype=float32)>,\n",
       " <tf.Variable 'linear5_40/Variable:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.14468786,  0.03440933, -0.01475575, -0.04427912,  0.01827583,\n",
       "        -0.10594981, -0.04964931, -0.00431582,  0.06513158, -0.00239601,\n",
       "        -0.04438701, -0.00421247,  0.03038071,  0.03752964, -0.07121366,\n",
       "         0.02298637, -0.03654976,  0.10602852, -0.01764759,  0.08015624,\n",
       "         0.03526419, -0.07197773,  0.00710227,  0.05912836, -0.00436876,\n",
       "        -0.02111311, -0.02590321, -0.09646378, -0.08870307, -0.09935635,\n",
       "         0.0489661 , -0.02985938], dtype=float32)>]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer5.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear5' object has no attribute 'calling'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6c87c39e3436>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinear_layer5\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcalling\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear5' object has no attribute 'calling'"
     ]
    }
   ],
   "source": [
    "linear_layer5.calling(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear4(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear4, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, a):\n",
    "        self.w = self.add_weight(shape=(a[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "\n",
    "    #def call(self, inputs):\n",
    "    #    return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "linear_layer_a = Linear_a(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\n",
      "call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=375, shape=(4, 32), dtype=float32, numpy=\n",
       "array([[-0.02419248,  0.03397185, -0.00721853, -0.03902208,  0.04057902,\n",
       "         0.08895342, -0.05935798,  0.15014562,  0.06483946, -0.05875566,\n",
       "        -0.17688638, -0.08090846,  0.00874879,  0.05344076,  0.11211172,\n",
       "        -0.02501448,  0.0373958 ,  0.00749506,  0.00742238,  0.10471216,\n",
       "        -0.04898893,  0.06628293, -0.10775416,  0.07727765, -0.10830477,\n",
       "         0.19399542, -0.0445637 , -0.07546984,  0.15959659, -0.03410715,\n",
       "         0.07729881,  0.06042442],\n",
       "       [-0.02419248,  0.03397185, -0.00721853, -0.03902208,  0.04057902,\n",
       "         0.08895342, -0.05935798,  0.15014562,  0.06483946, -0.05875566,\n",
       "        -0.17688638, -0.08090846,  0.00874879,  0.05344076,  0.11211172,\n",
       "        -0.02501448,  0.0373958 ,  0.00749506,  0.00742238,  0.10471216,\n",
       "        -0.04898893,  0.06628293, -0.10775416,  0.07727765, -0.10830477,\n",
       "         0.19399542, -0.0445637 , -0.07546984,  0.15959659, -0.03410715,\n",
       "         0.07729881,  0.06042442],\n",
       "       [-0.02419248,  0.03397185, -0.00721853, -0.03902208,  0.04057902,\n",
       "         0.08895342, -0.05935798,  0.15014562,  0.06483946, -0.05875566,\n",
       "        -0.17688638, -0.08090846,  0.00874879,  0.05344076,  0.11211172,\n",
       "        -0.02501448,  0.0373958 ,  0.00749506,  0.00742238,  0.10471216,\n",
       "        -0.04898893,  0.06628293, -0.10775416,  0.07727765, -0.10830477,\n",
       "         0.19399542, -0.0445637 , -0.07546984,  0.15959659, -0.03410715,\n",
       "         0.07729881,  0.06042442],\n",
       "       [-0.02419248,  0.03397185, -0.00721853, -0.03902208,  0.04057902,\n",
       "         0.08895342, -0.05935798,  0.15014562,  0.06483946, -0.05875566,\n",
       "        -0.17688638, -0.08090846,  0.00874879,  0.05344076,  0.11211172,\n",
       "        -0.02501448,  0.0373958 ,  0.00749506,  0.00742238,  0.10471216,\n",
       "        -0.04898893,  0.06628293, -0.10775416,  0.07727765, -0.10830477,\n",
       "         0.19399542, -0.0445637 , -0.07546984,  0.15959659, -0.03410715,\n",
       "         0.07729881,  0.06042442]], dtype=float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer_a(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SpZzAag4Mk_"
   },
   "source": [
    "The `__call__` method of your layer will automatically run `build` the first time it is called.\n",
    "You now have a layer that's lazy and easy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_cdMFCUp4KSQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=319, shape=(4, 32), dtype=float32, numpy=\n",
       "array([[-0.1274697 , -0.0630742 , -0.01142713,  0.05119533, -0.02470569,\n",
       "         0.05852924, -0.087138  ,  0.08649664,  0.04938974, -0.06793442,\n",
       "        -0.03455362,  0.02894306,  0.01817703,  0.02836488, -0.21493   ,\n",
       "         0.04592641, -0.0460737 , -0.05568846,  0.08812203, -0.16927117,\n",
       "        -0.12181335, -0.01016857,  0.09846251,  0.05953536, -0.04449662,\n",
       "         0.00659457,  0.02993122,  0.15447405, -0.07759735,  0.02221101,\n",
       "         0.02020011, -0.01443632],\n",
       "       [-0.1274697 , -0.0630742 , -0.01142713,  0.05119533, -0.02470569,\n",
       "         0.05852924, -0.087138  ,  0.08649664,  0.04938974, -0.06793442,\n",
       "        -0.03455362,  0.02894306,  0.01817703,  0.02836488, -0.21493   ,\n",
       "         0.04592641, -0.0460737 , -0.05568846,  0.08812203, -0.16927117,\n",
       "        -0.12181335, -0.01016857,  0.09846251,  0.05953536, -0.04449662,\n",
       "         0.00659457,  0.02993122,  0.15447405, -0.07759735,  0.02221101,\n",
       "         0.02020011, -0.01443632],\n",
       "       [-0.1274697 , -0.0630742 , -0.01142713,  0.05119533, -0.02470569,\n",
       "         0.05852924, -0.087138  ,  0.08649664,  0.04938974, -0.06793442,\n",
       "        -0.03455362,  0.02894306,  0.01817703,  0.02836488, -0.21493   ,\n",
       "         0.04592641, -0.0460737 , -0.05568846,  0.08812203, -0.16927117,\n",
       "        -0.12181335, -0.01016857,  0.09846251,  0.05953536, -0.04449662,\n",
       "         0.00659457,  0.02993122,  0.15447405, -0.07759735,  0.02221101,\n",
       "         0.02020011, -0.01443632],\n",
       "       [-0.1274697 , -0.0630742 , -0.01142713,  0.05119533, -0.02470569,\n",
       "         0.05852924, -0.087138  ,  0.08649664,  0.04938974, -0.06793442,\n",
       "        -0.03455362,  0.02894306,  0.01817703,  0.02836488, -0.21493   ,\n",
       "         0.04592641, -0.0460737 , -0.05568846,  0.08812203, -0.16927117,\n",
       "        -0.12181335, -0.01016857,  0.09846251,  0.05953536, -0.04449662,\n",
       "         0.00659457,  0.02993122,  0.15447405, -0.07759735,  0.02221101,\n",
       "         0.02020011, -0.01443632]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer = Linear(32)  # At instantiation, we don't know on what inputs this is going to get called\n",
    "linear_layer(x)  # The layer's weights are created dynamically the first time the layer is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear_a' object has no attribute 'w'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-698f5a4797d1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinear_layer_a\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear_a' object has no attribute 'w'"
     ]
    }
   ],
   "source": [
    "linear_layer_a.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'linear_3/Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
       "array([[-0.01024805, -0.12468822, -0.03961597, -0.03007054, -0.04628066,\n",
       "         0.04873963,  0.0056522 , -0.01616198, -0.00550074,  0.00910311,\n",
       "         0.05721447, -0.00032047,  0.02677893, -0.01647685,  0.02558899,\n",
       "         0.08199804,  0.01251215, -0.04133772,  0.00167426, -0.09411531,\n",
       "        -0.06747368, -0.03606477,  0.01028839, -0.09276644, -0.01903689,\n",
       "         0.00494483, -0.05151172, -0.10299905,  0.02904531,  0.00276534,\n",
       "         0.00856061, -0.03546946],\n",
       "       [ 0.09505925, -0.00867851, -0.00139748,  0.08212978,  0.1330924 ,\n",
       "        -0.02217632,  0.06656325, -0.0485464 , -0.04153347, -0.04293505,\n",
       "         0.01865369,  0.09250673, -0.03356451,  0.08733492, -0.03331181,\n",
       "         0.00075316,  0.11341518,  0.01956592, -0.02479024, -0.04311122,\n",
       "         0.03003897,  0.04169715,  0.12543605, -0.014     ,  0.00981854,\n",
       "         0.08322202,  0.08934832,  0.01648777,  0.0848069 , -0.02683881,\n",
       "         0.06585511,  0.01475406]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'linear_a_1/Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
       "array([[-0.02224136,  0.10893494,  0.01650461,  0.04596733, -0.00191518,\n",
       "        -0.00289454,  0.01686579, -0.03833527, -0.05119411, -0.0350647 ,\n",
       "         0.00085375,  0.04506344,  0.0066293 ,  0.04897112,  0.0204167 ,\n",
       "         0.02053897,  0.00276471, -0.07860785,  0.05259438, -0.0493194 ,\n",
       "        -0.12971416, -0.02451086,  0.01026036,  0.07726505,  0.03485465,\n",
       "         0.02859635, -0.07592668, -0.03255811,  0.0422284 ,  0.08190619,\n",
       "        -0.0173349 , -0.0686255 ],\n",
       "       [ 0.02413127, -0.01974991,  0.03357071,  0.04227452, -0.06001312,\n",
       "        -0.05494372,  0.05127419, -0.00933078,  0.01657465, -0.05440009,\n",
       "         0.03437327,  0.01573649,  0.00602964,  0.02063557,  0.00344559,\n",
       "        -0.00656494,  0.09053495,  0.10931852, -0.1267864 ,  0.02341956,\n",
       "        -0.08000993, -0.04601313, -0.05010744,  0.01067231, -0.00597965,\n",
       "         0.07272794, -0.04900721, -0.00134051,  0.02059883, -0.06399708,\n",
       "        -0.04932237,  0.02444435]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer_a = Linear_a(32)\n",
    "y = linear_layer_a(x)\n",
    "linear_layer_a.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-4fee993bfc6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlinear_layer3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinear3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_layer3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    885\u001b[0m         \u001b[1;31m# Eager execution on data tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 887\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    888\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2139\u001b[0m         \u001b[1;31m# operations.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2140\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2141\u001b[1;33m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2142\u001b[0m       \u001b[1;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2143\u001b[0m       \u001b[1;31m# constrained to set self.built.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: build() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "linear_layer3 = Linear3(32)\n",
    "y = linear_layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'linear4_4/Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
       "array([[ 0.02944901,  0.08169571,  0.08607452,  0.09525241,  0.03352685,\n",
       "         0.00728847, -0.04876156, -0.07203896,  0.03152307,  0.02647154,\n",
       "        -0.09575149, -0.03901853, -0.11773364,  0.02425112, -0.00497574,\n",
       "         0.00120795, -0.02768264, -0.05510433,  0.05402781, -0.0268576 ,\n",
       "        -0.0396144 , -0.07633492, -0.01152158,  0.03471149, -0.04223849,\n",
       "         0.03975738,  0.09698174, -0.03637597,  0.00842177, -0.03485252,\n",
       "        -0.05574738,  0.04095755],\n",
       "       [-0.05322326, -0.03908886,  0.04748499,  0.01411388,  0.00665069,\n",
       "         0.08479541,  0.05209041, -0.04581574,  0.0448983 ,  0.00983533,\n",
       "         0.05158487,  0.04326131,  0.02078844,  0.06170132, -0.04357408,\n",
       "         0.04804098,  0.02756624,  0.08641037, -0.00463737, -0.0491388 ,\n",
       "        -0.05058886,  0.0049554 , -0.0373957 ,  0.00160119,  0.0137968 ,\n",
       "        -0.08824806, -0.01923079,  0.09739023,  0.06208529, -0.03312061,\n",
       "         0.02554913, -0.01775558]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer4 = Linear4(32)\n",
    "y = linear_layer4(x)\n",
    "linear_layer4.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'inputs_shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-1fb8e030b500>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinear_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'inputs_shape'"
     ]
    }
   ],
   "source": [
    "linear_layer.inputs_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-kaDooBSC_Oc"
   },
   "source": [
    "\n",
    "### Layers are recursively composable\n",
    "\n",
    "If you assign a Layer instance as attribute of another Layer,\n",
    "the outer layer will start tracking the weights of the inner layer.\n",
    "\n",
    "We recommend creating such sublayers in the `__init__` method (since the sublayers will typically have a `build` method, they will be built when the outer layer gets built)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YPI4vwN4Ozo",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "init\n",
      "init\n",
      "build\n",
      "call\n",
      "build\n",
      "call\n",
      "build\n",
      "call\n",
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "# Let's assume we are reusing the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "class MLPBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "print('weights:', len(mlp.weights))\n",
    "print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "class MLPBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3, 64)))  # The first call to the `mlp` will create the weights\n",
    "print('weights:', len(mlp.weights))\n",
    "print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'mlp_block/linear_7/Variable:0' shape=(64, 32) dtype=float32, numpy=\n",
       " array([[ 0.0180655 ,  0.02374338, -0.05606483, ..., -0.01315719,\n",
       "         -0.03572294, -0.01693827],\n",
       "        [ 0.00323544, -0.04751184,  0.07704943, ...,  0.01181477,\n",
       "         -0.00795043, -0.00286257],\n",
       "        [ 0.0608725 ,  0.05619473,  0.09616824, ..., -0.04533891,\n",
       "          0.02907863,  0.11917132],\n",
       "        ...,\n",
       "        [ 0.0464141 ,  0.04623416,  0.0678204 , ...,  0.07510582,\n",
       "         -0.02357451, -0.05712678],\n",
       "        [ 0.04974559,  0.00750397, -0.02929542, ...,  0.04593847,\n",
       "          0.02657567,  0.0264814 ],\n",
       "        [ 0.01923304,  0.01592487,  0.02636321, ..., -0.008432  ,\n",
       "          0.00454803,  0.0552357 ]], dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_7/Variable:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.02736011,  0.03980467,  0.00797743,  0.05097401,  0.09139656,\n",
       "         0.08187187, -0.00299348,  0.00110982,  0.04128009, -0.03663872,\n",
       "        -0.10554752, -0.00448293, -0.00175833,  0.04130923,  0.00615854,\n",
       "         0.00872232, -0.0483223 ,  0.00915977,  0.01082436, -0.01484789,\n",
       "        -0.0300011 , -0.05374504, -0.01208897,  0.02449537,  0.09224094,\n",
       "         0.02333268, -0.09839249, -0.07497966, -0.07034629, -0.05064901,\n",
       "         0.09096285,  0.08266085], dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_8/Variable:0' shape=(32, 32) dtype=float32, numpy=\n",
       " array([[ 0.01967924,  0.00697875,  0.03034665, ...,  0.08690856,\n",
       "          0.03772469,  0.07593927],\n",
       "        [-0.04224731, -0.05191275,  0.01814629, ..., -0.00151451,\n",
       "         -0.07033085, -0.07242125],\n",
       "        [ 0.094359  , -0.04207202, -0.07602578, ...,  0.01522134,\n",
       "         -0.02022033, -0.07703578],\n",
       "        ...,\n",
       "        [ 0.03343345,  0.02532764, -0.03364972, ...,  0.01002511,\n",
       "          0.01768664,  0.08209225],\n",
       "        [-0.07469888,  0.01716298, -0.08236887, ...,  0.04901778,\n",
       "         -0.04354519,  0.12300947],\n",
       "        [ 0.08086973, -0.13310365, -0.00299664, ...,  0.03265407,\n",
       "          0.05216102,  0.0330857 ]], dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_8/Variable:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-1.6042586e-02, -1.7782187e-02, -5.9217210e-03,  2.8456179e-02,\n",
       "        -6.8103887e-05,  3.3903740e-02,  4.2588238e-02,  6.6516839e-02,\n",
       "        -2.5157342e-02, -4.0420733e-02, -5.6347907e-02,  3.1960282e-02,\n",
       "        -1.5234573e-04,  5.1809665e-02,  2.3530484e-03,  1.8248387e-02,\n",
       "         2.5746331e-02, -1.5554401e-02, -1.9444160e-02, -5.9739628e-04,\n",
       "         1.4871969e-02, -6.2762588e-02, -3.5046984e-02, -4.0525176e-02,\n",
       "         5.9703719e-02,  3.1604674e-02,  7.1318462e-02, -4.1236956e-02,\n",
       "         3.7110530e-02,  2.1454815e-02,  1.6482990e-02,  6.2689714e-02],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_9/Variable:0' shape=(32, 2) dtype=float32, numpy=\n",
       " array([[ 0.04302071, -0.03616288],\n",
       "        [ 0.08790111, -0.02941172],\n",
       "        [-0.07553247, -0.00345302],\n",
       "        [-0.00363206, -0.00793749],\n",
       "        [ 0.03127579, -0.02413955],\n",
       "        [ 0.02326309, -0.02919407],\n",
       "        [-0.04356423, -0.04512827],\n",
       "        [ 0.02491898,  0.08071765],\n",
       "        [-0.00877535,  0.01571397],\n",
       "        [ 0.05177122, -0.01504706],\n",
       "        [ 0.06462423,  0.02124491],\n",
       "        [-0.07423999,  0.08395135],\n",
       "        [-0.05857379,  0.04506074],\n",
       "        [ 0.0561091 ,  0.06502625],\n",
       "        [ 0.05821138, -0.02472389],\n",
       "        [-0.06489503, -0.11918417],\n",
       "        [-0.01139493, -0.10614665],\n",
       "        [ 0.05368064,  0.08139484],\n",
       "        [-0.01210607, -0.05105522],\n",
       "        [-0.0325252 , -0.02050435],\n",
       "        [ 0.05552913,  0.03800423],\n",
       "        [-0.03145863,  0.07828576],\n",
       "        [ 0.0492857 , -0.00536544],\n",
       "        [-0.01225222, -0.04778271],\n",
       "        [-0.09233536,  0.02367016],\n",
       "        [-0.0334309 ,  0.04504454],\n",
       "        [-0.03832081,  0.00481806],\n",
       "        [-0.01358052, -0.01589331],\n",
       "        [ 0.07622447, -0.03667111],\n",
       "        [ 0.01546991, -0.0544524 ],\n",
       "        [-0.07920067,  0.03394043],\n",
       "        [-0.01430089, -0.06508829]], dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_9/Variable:0' shape=(2,) dtype=float32, numpy=array([-0.06158426,  0.03215369], dtype=float32)>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.09285626 -0.02166161]\n",
      " [ 0.09285626 -0.02166161]\n",
      " [ 0.09285626 -0.02166161]], shape=(3, 2), dtype=float32) tf.Tensor(\n",
      "[[ 0.17987949 -0.04317725]\n",
      " [ 0.17987949 -0.04317725]\n",
      " [ 0.17987949 -0.04317725]], shape=(3, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(y,y + mlp.weights[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=427, shape=(4, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.],\n",
       "       [1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'mlp_block_1/linear_11/Variable:0' shape=(2,) dtype=float32, numpy=array([ 0.08702324, -0.02151564], dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.weights[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The layer has never been called and thus has no defined input shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-890358de653a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36minput_shape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1594\u001b[0m     \"\"\"\n\u001b[0;32m   1595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inbound_nodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1596\u001b[1;33m       raise AttributeError('The layer has never been called '\n\u001b[0m\u001b[0;32m   1597\u001b[0m                            'and thus has no defined input shape.')\n\u001b[0;32m   1598\u001b[0m     all_input_shapes = set(\n",
      "\u001b[1;31mAttributeError\u001b[0m: The layer has never been called and thus has no defined input shape."
     ]
    }
   ],
   "source": [
    "a = Linear(32)\n",
    "b = Linear()\n",
    "z = a(tf.ones(shape=(3, 64)))\n",
    "a.input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fq5_AsbEh-BQ"
   },
   "source": [
    "### Layers recursively collect losses created during the forward pass\n",
    "\n",
    "When writing the `call` method of a layer, you can create loss tensors that you will want to use later, when writing your training loop. This is doable by calling `self.add_loss(value)`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W66HsCzajERu"
   },
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularization loss\n",
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p_dMrJ-QjcZH"
   },
   "source": [
    "These losses (including those created by any inner layer) can be retrieved via `layer.losses`.\n",
    "This property is reset at the start of every `__call__` to the top-level layer, so that `layer.losses` always contains the loss values created during the last forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1vYiSnVjdCc"
   },
   "outputs": [],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0  # No losses yet since the layer has never been called\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # We created one loss value\n",
    "\n",
    "# `layer.losses` gets reset at the start of each __call__\n",
    "_ = layer(tf.zeros(1, 1))\n",
    "assert len(layer.losses) == 1  # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=45, shape=(), dtype=float32, numpy=0.0>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(layer.losses)\n",
    "len(layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=52, shape=(), dtype=float32, numpy=0.04>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_ = layer(tf.ones((2,2)))\n",
    "layer.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Jv3LKNfk_LL"
   },
   "source": [
    "In addition, the `loss` property also contains regularization losses created for the weights of any inner layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iokhhZfUlJUU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=1329, shape=(), dtype=float32, numpy=0.0017187007>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.dense = layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "\n",
    "\n",
    "layer = OuterLayer()\n",
    "_ = layer(tf.zeros((1, 1)))\n",
    "\n",
    "# This is `1e-3 * sum(layer.dense.kernel ** 2)`,\n",
    "# created by the `kernel_regularizer` above.\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(test, self).__init__()\n",
    "        self.dense = layers.Dense(32)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "    \n",
    "layer_test = test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_test.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "_ = layer_test(tf.ones((1,1)))\n",
    "print(layer_test.weights[0].initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'outer_layer_1/dense/kernel:0' shape=(1, 32) dtype=float32, numpy=\n",
       "array([[ 0.20301062,  0.41947216, -0.3077877 ,  0.3320356 , -0.04775584,\n",
       "         0.09079772, -0.24240516,  0.29237407, -0.01459622,  0.1882621 ,\n",
       "        -0.21656553, -0.28707606, -0.20224887,  0.32503098, -0.39885205,\n",
       "         0.09833658, -0.17787234, -0.36508054,  0.3141998 ,  0.24522626,\n",
       "         0.03932247,  0.12195104, -0.26490855,  0.2911014 ,  0.0182344 ,\n",
       "        -0.21591367, -0.38121134,  0.3878811 ,  0.21664739, -0.28069466,\n",
       "         0.40381426, -0.36430547]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weights[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2Xdp_dvlGLG"
   },
   "source": [
    "These losses are meant to be taken into account when writing training loops, like this:\n",
    "\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2Xdp_dvlGLG"
   },
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = layer(x_batch_train)  # Logits for this minibatch\n",
    "    # Loss value for this minibatch\n",
    "    loss_value = loss_fn(y_batch_train, logits)\n",
    "    # Add extra losses created during this forward pass:\n",
    "    loss_value += sum(model.losses)\n",
    "\n",
    "  grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2Xdp_dvlGLG"
   },
   "source": [
    "```\n",
    "\n",
    "For a detailed guide about writing training loops, see the second section of the [guide to training and evaluation](./train_and_evaluate.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are Numpy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "def get_uncompiled_model():\n",
    "    inputs = keras.Input(shape=(784,), name='digits')\n",
    "    x = layers.Dense(64, activation='relu', name='dense_1')(inputs)\n",
    "    x = layers.Dense(64, activation='relu', name='dense_2')(x)\n",
    "    outputs = layers.Dense(10, name='predictions')(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "def get_compiled_model():\n",
    "    model = get_uncompiled_model()\n",
    "    model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['sparse_categorical_accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_compiled_model()\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=655, shape=(1, 32), dtype=float32, numpy=\n",
       "array([[ 0.20301062,  0.41947216, -0.3077877 ,  0.3320356 , -0.04775584,\n",
       "         0.09079772, -0.24240516,  0.29237407, -0.01459622,  0.1882621 ,\n",
       "        -0.21656553, -0.28707606, -0.20224887,  0.32503098, -0.39885205,\n",
       "         0.09833658, -0.17787234, -0.36508054,  0.3141998 ,  0.24522626,\n",
       "         0.03932247,  0.12195104, -0.26490855,  0.2911014 ,  0.0182344 ,\n",
       "        -0.21591367, -0.38121134,  0.3878811 ,  0.21664739, -0.28069466,\n",
       "         0.40381426, -0.36430547]], dtype=float32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(tf.ones((1,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "?layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Matrix size-incompatible: In[0]: [64,784], In[1]: [1,32] [Op:MatMul]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-dbd72138abf4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch_train\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Logits for this minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;31m# Loss value for this minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-b5e22aff1b2c>\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    889\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[0;32m    890\u001b[0m               self._compute_dtype):\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   1054\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msparse_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse_tensor_dense_matmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias_add\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[1;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[0;32m   6124\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6125\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6126\u001b[1;33m       \u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6127\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6128\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtranspose_a\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tf2.0-gpu\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Matrix size-incompatible: In[0]: [64,784], In[1]: [1,32] [Op:MatMul]"
     ]
    }
   ],
   "source": [
    "# Instantiate an optimizer\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = layer(x_batch_train) # Logits for this minibatch\n",
    "        # Loss value for this minibatch\n",
    "        loss_value = loss_fn(y_batch_train, logits)\n",
    "        # Add extra losses created during this forward pass:\n",
    "        loss_value += sum(model.losses)\n",
    "        \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ozo04iqHohNg"
   },
   "source": [
    "### You can optionally enable serialization on your layers\n",
    "\n",
    "If you need your custom layers to be serializable as part of a [Functional model](./functional.ipynb), you can optionally implement a `get_config` method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckT5Zbo0oxrz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape = (input_shape[-1], self.units),\n",
    "                                 initializer = 'random_normal',\n",
    "                                 trainable = True)\n",
    "        self.b = self.add_weight(shape = (self.units,),\n",
    "                                 initializer = 'random_normal',\n",
    "                                 trainable = True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        return {'units': self.units}\n",
    "\n",
    "\n",
    "# Now you can recreate the layer from its config:\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fKngh4UozyM"
   },
   "source": [
    "Note that the `__init__` method of the base `Layer` class takes some keyword arguments, in particular a `name` and a `dtype`. It's good practice to pass these arguments to the parent class in `__init__` and to include them in the layer config:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UCMoN42no0D5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_14', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape = (input_shape[-1], self.units),\n",
    "                                 initializer = 'random_normal',\n",
    "                                 trainable = True)\n",
    "        self.b = self.add_weight(shape = (self.units,),\n",
    "                                 initializer = 'random_normal',\n",
    "                                 trainable = True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units': self.units})\n",
    "        return config\n",
    "\n",
    "\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "#new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNrHV0zAo0Tc"
   },
   "source": [
    "If you need more flexibility when deserializing the layer from its config, you can also override the `from_config` class method. This is the base implementation of `from_config`:\n",
    "\n",
    "```python\n",
    "def from_config(cls, config):\n",
    "  return cls(**config)\n",
    "```\n",
    "\n",
    "To learn more about serialization and saving, see the complete [Guide to Saving and Serializing Models](./save_and_serialize.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TB8iViSo4p9"
   },
   "source": [
    "### Privileged `training` argument in the `call` method\n",
    "\n",
    "\n",
    "Some layers, in particular the `BatchNormalization` layer and the `Dropout` layer, have different behaviors during training and inference. For such layers, it is standard practice to expose a `training` (boolean) argument in the `call` method.\n",
    "\n",
    "By exposing this argument in `call`, you enable the built-in training and evaluation loops (e.g. `fit`) to correctly use the layer in training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QyI_b4Rgo-EE"
   },
   "outputs": [],
   "source": [
    "class CustomDropout(layers.Layer):\n",
    "\n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate=self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3X6eQH_K2wf1"
   },
   "source": [
    "## Building Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZen-bAOE9I5"
   },
   "source": [
    "\n",
    "\n",
    "### The Model class\n",
    "\n",
    "In general, you will use the `Layer` class to define inner computation blocks,\n",
    "and will use the `Model` class to define the outer model -- the object you will train.\n",
    "\n",
    "For instance, in a ResNet50 model, you would have several ResNet blocks subclassing `Layer`,\n",
    "and a single `Model` encompassing the entire ResNet50 network.\n",
    "\n",
    "The `Model` class has the same API as `Layer`, with the following differences:\n",
    "\n",
    "- It exposes built-in training, evaluation, and prediction loops (`model.fit()`, `model.evaluate()`, `model.predict()`).\n",
    "- It exposes the list of its inner layers, via the `model.layers` property.\n",
    "- It exposes saving and serialization APIs.\n",
    "\n",
    "Effectively, the \"Layer\" class corresponds to what we refer to in the literature\n",
    "as a \"layer\" (as in \"convolution layer\" or \"recurrent layer\") or as a \"block\" (as in \"ResNet block\" or \"Inception block\").\n",
    "\n",
    "Meanwhile, the \"Model\" class corresponds to what is referred to in the literature\n",
    "as a \"model\" (as in \"deep learning model\") or as a \"network\" (as in \"deep neural network\").\n",
    "\n",
    "For instance, we could take our mini-resnet example above, and use it to build a `Model` that we could\n",
    "train with `fit()`, and that we could save with `save_weights`:\n",
    "\n",
    "```python\n",
    "class ResNet(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs=10)\n",
    "resnet.save_weights(filepath)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dense' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-691856376d85>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mresnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mresnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-100-691856376d85>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m#self.block_2 = ResnetBlock()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dense' is not defined"
     ]
    }
   ],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResnetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs = 10)\n",
    "resnet.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "roVCX-TJqYzx"
   },
   "source": [
    "### Putting it all together: an end-to-end example\n",
    "\n",
    "Here's what you've learned so far:\n",
    "\n",
    "- A `Layer` encapsulate a state (created in `__init__` or `build`) and some computation (in `call`).\n",
    "- Layers can be recursively nested to create new, bigger computation blocks.\n",
    "- Layers can create and track losses (typically regularization losses).\n",
    "- The outer container, the thing you want to train, is a `Model`. A `Model` is just like a `Layer`, but with added training and serialization utilities.\n",
    "\n",
    "Let's put all of these things together into an end-to-end example: we're going to implement a Variational AutoEncoder (VAE). We'll train it on MNIST digits.\n",
    "\n",
    "Our VAE will be a subclass of `Model`, built as a nested composition of layers that subclass `Layer`. It will feature a regularization loss (KL divergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QxkfjtzE4X2"
   },
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape = (batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                latent_dim = 32,\n",
    "                intermediate_dim = 64,\n",
    "                name = 'encoder',\n",
    "                **kwargs):\n",
    "        super(Encoder, self).__init__(name = name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation = 'relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "\n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                original_dim,\n",
    "                intermediate_dim = 64,\n",
    "                name = 'decoder',\n",
    "                **kwargs):\n",
    "        super(Decoder, self).__init__(name = name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation = 'relu')\n",
    "        self.dense_output = layers.Dense(original_dim, activation = 'sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        return self.dense_output(x)\n",
    "\n",
    "\n",
    "class VariationalAutoEncoder(tf.keras.Model):\n",
    "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                original_dim,\n",
    "                intermediate_dim = 64,\n",
    "                latent_dim = 32,\n",
    "                name = 'autoencoder',\n",
    "                **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name = name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim = latent_dim,\n",
    "                               intermediate_dim = intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim = intermediate_dim)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = - 0.5 * tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDVSVl4Iu8kC",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 0\n",
      "step 0: mean loss = tf.Tensor(0.34416658, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.12549832, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.09920556, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.08919379, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.084206045, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.08087842, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.07874563, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07715198, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.075982735, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.07496637, shape=(), dtype=float32)\n",
      "Start of epoch 1\n",
      "step 0: mean loss = tf.Tensor(0.074673206, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.07402148, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.073512234, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.07303131, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.07270429, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.07229924, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.07200857, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07171704, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.0714686, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.07120582, shape=(), dtype=float32)\n",
      "Start of epoch 2\n",
      "step 0: mean loss = tf.Tensor(0.071135096, shape=(), dtype=float32)\n",
      "step 100: mean loss = tf.Tensor(0.070957854, shape=(), dtype=float32)\n",
      "step 200: mean loss = tf.Tensor(0.07083418, shape=(), dtype=float32)\n",
      "step 300: mean loss = tf.Tensor(0.07067346, shape=(), dtype=float32)\n",
      "step 400: mean loss = tf.Tensor(0.0705809, shape=(), dtype=float32)\n",
      "step 500: mean loss = tf.Tensor(0.07042731, shape=(), dtype=float32)\n",
      "step 600: mean loss = tf.Tensor(0.07030696, shape=(), dtype=float32)\n",
      "step 700: mean loss = tf.Tensor(0.07018819, shape=(), dtype=float32)\n",
      "step 800: mean loss = tf.Tensor(0.07008608, shape=(), dtype=float32)\n",
      "step 900: mean loss = tf.Tensor(0.069963045, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# Iterate over epochs.\n",
    "for epoch in range(epochs):\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "    # Iterate over the batches of the dataset.\n",
    "    for step, x_batch_train in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            reconstructed = vae(x_batch_train)\n",
    "            # Compute reconstruction loss\n",
    "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
    "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
    "\n",
    "        grads = tape.gradient(loss, vae.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "        loss_metric(loss)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print('step %s: mean loss = %s' % (step, loss_metric.result()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TensorSliceDataset shapes: (784,), types: tf.float32>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "#train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)\n",
    "x_train\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5hgOl_y34NZD"
   },
   "source": [
    "Note that since the VAE is subclassing `Model`, it features built-in training loops. So you could also have trained it like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y153oEzk4Piz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 45us/sample - loss: 0.0749\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0676\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x238e0eff6c8>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae = VariationalAutoEncoder(784, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "\n",
    "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs = 3, batch_size = 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZVkFme-U0IHb"
   },
   "source": [
    "### Beyond object-oriented development: the Functional API\n",
    "\n",
    "Was this example too much object-oriented development for you? You can also build models using [the Functional API](./functional.ipynb). Importantly, choosing one style or another does not prevent you from leveraging components written in the other style: you can always mix-and-match.\n",
    "\n",
    "For instance, the Functional API example below reuses the same `Sampling` layer we defined in the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QzeXGAl3Uxn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 3s 56us/sample - loss: 0.0745\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0676\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 3s 47us/sample - loss: 0.0675\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa0d0323198>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dim = 784\n",
    "intermediate_dim = 64\n",
    "latent_dim = 32\n",
    "\n",
    "# Define encoder model.\n",
    "original_inputs = tf.keras.Input(shape = (original_dim,), name = 'encoder_input')\n",
    "x = layers.Dense(intermediate_dim, activation = 'relu')(original_inputs)\n",
    "z_mean = layers.Dense(latent_dim, name = 'z_mean')(x)\n",
    "z_log_var = layers.Dense(latent_dim, name = 'z_log_var')(x)\n",
    "z = Sampling()((z_mean, z_log_var))\n",
    "encoder = tf.keras.Model(inputs = original_inputs, outputs = z, name = 'encoder')\n",
    "\n",
    "# Define decoder model.\n",
    "latent_inputs = tf.keras.Input(shape = (latent_dim,), name = 'z_sampling')\n",
    "x = layers.Dense(intermediate_dim, activation = 'relu')(latent_inputs)\n",
    "outputs = layers.Dense(original_dim, activation = 'sigmoid')(x)\n",
    "decoder = tf.keras.Model(inputs = latent_inputs, outputs = outputs, name = 'decoder')\n",
    "\n",
    "# Define VAE model.\n",
    "outputs = decoder(z)\n",
    "vae = tf.keras.Model(inputs = original_inputs, outputs = outputs, name = 'vae')\n",
    "\n",
    "# Add KL divergence regularization loss.\n",
    "kl_loss = - 0.5 * tf.reduce_mean(\n",
    "    z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "vae.add_loss(kl_loss)\n",
    "\n",
    "# Train.\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "vae.compile(optimizer, loss = tf.keras.losses.MeanSquaredError())\n",
    "vae.fit(x_train, x_train, epochs = 3, batch_size = 64)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "custom_layers_and_models.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
