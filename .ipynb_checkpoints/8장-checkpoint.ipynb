{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session() # For easy reset of notebook state.\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.05653545  0.03444588 -0.01249578 -0.00676338]\n",
      " [ 0.05653545  0.03444588 -0.01249578 -0.00676338]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32, input_dim = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value = w_init(shape=(input_dim, units),\n",
    "                                                    dtype = 'float32'),\n",
    "                            trainable = True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value = b_init(shape=(units,),\n",
    "                                                   dtype='float32'),\n",
    "                            trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.04208444 -0.11881118  0.03900589 -0.05673023]\n",
      " [-0.04208444 -0.11881118  0.03900589 -0.05673023]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units = 32, input_dim = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape = (input_dim, units),\n",
    "                                initializer = 'random_normal',\n",
    "                                trainable = True)\n",
    "        self.b = self.add_weight(shape = (units, ),\n",
    "                                initializer = 'zeros',\n",
    "                                trainable = True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "linear_layer = Linear(4, 2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2.]\n",
      "[4. 4.]\n"
     ]
    }
   ],
   "source": [
    "class ComputeSum(layers.Layer):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        self.total = tf.Variable(initial_value = tf.zeros((input_dim, )),\n",
    "                                trainable = False)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis = 0))\n",
    "        return self.total\n",
    "\n",
    "x = tf.ones((2, 2))\n",
    "my_sum = ComputeSum(2)\n",
    "y = my_sum(x)\n",
    "print(y.numpy())\n",
    "y = my_sum(x)\n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 1\n",
      "non-trainable weights: 1\n",
      "trainable_weights: []\n"
     ]
    }
   ],
   "source": [
    "print('weights:',len(my_sum.weights))\n",
    "print('non-trainable weights:', len(my_sum.non_trainable_weights))\n",
    "\n",
    "#It's not included in the trainable weights:\n",
    "print('trainable_weights:', my_sum.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units = 32, input_dim = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.w = self.add_weight(shape = (input_dim, units),\n",
    "                                initializer = 'random_normal',\n",
    "                                trainable = True)\n",
    "        self.b = self.add_weight(shape = (units, ),\n",
    "                                 initializer = 'zeros',\n",
    "                                 trainable = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape = (input_shape[-1], self.units),\n",
    "                                initializer = 'random_normal',\n",
    "                                trainable = True)\n",
    "        self.b = self.add_weight(shape = (self.units, ),\n",
    "                                initializer = 'random_normal',\n",
    "                                trainable = True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_layer = Linear(32) \n",
    "# At instantiation, we don't know on what inputs this going to get called\n",
    "\n",
    "y = linear_layer(x)\n",
    "# The layer's weights are created dynamically the first time the layer is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: 6\n",
      "trainable weights: 6\n"
     ]
    }
   ],
   "source": [
    "# Let's assume we are reusing the Linear class\n",
    "# with a 'build' method that we defined above.\n",
    "\n",
    "class MLPBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MLPBlock, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "    \n",
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape = (3, 64))) # The first call to the 'mlp' will create the weights\n",
    "print('weights:', len(mlp.weights))\n",
    "print('trainable weights:', len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A layer that creates an activity regularization loss\n",
    "class ActivityRegularizationLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, rate = 1e-2):\n",
    "        super(ActivityRegularizationLayer, self).__init__()\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.activity_reg = ActivityRegularizationLayer(1e-2)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.activity_reg(inputs)\n",
    "    \n",
    "layer = OuterLayer()\n",
    "assert len(layer.losses) == 0 # No losses yet since the layer has never been called\n",
    "_ = layer(tf.zeros(1,1))\n",
    "assert len(layer.losses) == 1 # We created one loss value\n",
    "\n",
    "# 'layer.losses' gets reset at the start of each __call__\n",
    "_ = layer(tf.zeros(1,1))\n",
    "assert len(layer.losses) == 1 # This is the loss created during the call above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=323, shape=(), dtype=float32, numpy=0.0022824414>]\n"
     ]
    }
   ],
   "source": [
    "class OuterLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(OuterLayer, self).__init__()\n",
    "        self.dense = layers.Dense(32, kernel_regularizer = tf.keras.regularizers.l2(1e-3))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return self.dense(inputs)\n",
    "    \n",
    "layer = OuterLayer()\n",
    "_ = layer(tf.zeros((1,1)))\n",
    "\n",
    "# This is '1e-3 * sum(layer.dense.kernel ** 2)',\n",
    "# created by the 'kernel_regularizer' above.\n",
    "print(layer.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-c9135f39ca34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Iterate over the batches of a dataset.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mx_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch_train\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Logits for this minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits = True)\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for x_batch_train, y_batch_train in train_dataset:\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = layer(x_batch_train) # Logits for this minibatch\n",
    "        # Loss value for this minibatch\n",
    "        loss_value = loss_fn(y_batch_train, logits)\n",
    "        # Add extra losses created during this forward pass:\n",
    "        loss_value += sum(model.losses)\n",
    "        \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units = 32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape = (input_shape[-1], self.units),\n",
    "                                initializer = 'random_normal',\n",
    "                                trainable = True)\n",
    "        self.b = self.add_weight(shape = (self.units, ),\n",
    "                                initializer = 'random_normal',\n",
    "                                trainable = True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'units' : self.units}\n",
    "    \n",
    "# Now you can recreate the layer from its config:\n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'linear_2', 'trainable': True, 'dtype': 'float32', 'units': 64}\n"
     ]
    }
   ],
   "source": [
    "class Linear(layers.Layer):\n",
    "    \n",
    "    def __init__(self, units = 32, **kwargs):\n",
    "        super(Linear, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape = (input_shape[-1], self.units),\n",
    "                                initializer = 'random_normal',\n",
    "                                trainable = True)\n",
    "        self.b = self.add_weight(shape = (self.units, ),\n",
    "                                initializer = 'random_normal',\n",
    "                                trainable = True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(Linear, self).get_config()\n",
    "        config.update({'units' : self.units})\n",
    "        return config\n",
    "    \n",
    "layer = Linear(64)\n",
    "config = layer.get_config()\n",
    "print(config)\n",
    "new_layer = Linear.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_config(cls, config):\n",
    "    return cls(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(layers.Layer):\n",
    "    \n",
    "    def __init__(self, rate, **kwargs):\n",
    "        super(CustomDropout, self).__init__(**kwargs)\n",
    "        self.rate = rate\n",
    "        \n",
    "    def call(self, inputs, training = None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs, rate = self.rate)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNetBlock' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-2258f2bf8d11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mresnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mresnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-2258f2bf8d11>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mResNet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNetBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblock_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mResNetBlock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_pool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ResNetBlock' is not defined"
     ]
    }
   ],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.block_1 = ResNetBlock()\n",
    "        self.block_2 = ResNetBlock()\n",
    "        self.global_pool = layers.GlobalAveragePooling2D()\n",
    "        self.classifier = Dense(num_classes)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.block_1(inputs)\n",
    "        x = self.block_2(x)\n",
    "        x = self.global_pool(x)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "resnet = ResNet()\n",
    "dataset = ...\n",
    "resnet.fit(dataset, epochs = 10)\n",
    "resnet.save_weights(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape = (batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "    \n",
    "class Encoder(layers.Layer):\n",
    "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                latent_dim = 32,\n",
    "                 intermediate_dim = 64,\n",
    "                 name = 'encoder',\n",
    "                 **kwargs):\n",
    "        super(Encoder, self).__init__(name=name, **kwargs)\n",
    "        self.dense_proj = layers.Dense(intermediate_dim, activation = 'relu')\n",
    "        self.dense_mean = layers.Dense(latent_dim)\n",
    "        self.dense_log_var = layers.Dense(latent_dim)\n",
    "        self.sampling = Sampling()\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense_proj(inputs)\n",
    "        z_mean = self.dense_mean(x)\n",
    "        z_log_var = self.dense_log_var(x)\n",
    "        z = self.sampling((z_mean, z_log_var))\n",
    "        return z_mean, z_log_var, z\n",
    "    \n",
    "class Decoder(layers.Layer):\n",
    "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                original_dim,\n",
    "                intermediate_dim = 64,\n",
    "                name = 'decoder',\n",
    "                **kwargs):\n",
    "        super(VariationalAutoEncoder, self).__init__(name=name, **kwargs)\n",
    "        self.original_dim = original_dim\n",
    "        self.encoder = Encoder(latent_dim = latent_dim,\n",
    "                               intermediate_dim = intermediate_dim)\n",
    "        self.decoder = Decoder(original_dim, intermediate_dim = intermediate_dim)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var, z = self.encoder(inputs)\n",
    "        reconstructed = self.decoder(z)\n",
    "        # Add KL divergence regularization loss.\n",
    "        kl_loss = -0.5*tf.reduce_mean(\n",
    "            z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
    "        self.add_loss(kl_loss)\n",
    "        return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = 784\n",
    "vae = VariationalAutoEncoder(original_dim, 64, 32)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-3)\n",
    "mse_loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "\n",
    "(x_train, _), _ = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear2(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear2, self).__init__()\n",
    "        self.units = units\n",
    "        print(\"init\")\n",
    "\n",
    "    def call(self, defg):\n",
    "        print(\"call\")\n",
    "        return tf.matmul(defg, self.w) + self.b\n",
    "\n",
    "    def build(self, abc):\n",
    "        self.w = self.add_weight(shape=(abc[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        print(\"build\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "linear_layer2 = Linear2(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build\n",
      "call\n"
     ]
    }
   ],
   "source": [
    "y=linear_layer2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=158, shape=(2, 32), dtype=float32, numpy=\n",
       "array([[-0.06849635,  0.00995546,  0.05816585,  0.03781942, -0.02526987,\n",
       "         0.05539752, -0.17635791,  0.05470857,  0.17366175,  0.01917322,\n",
       "         0.06387094, -0.01698533, -0.00506841, -0.00584116,  0.07189137,\n",
       "        -0.06351052, -0.00188681, -0.08475505, -0.05147968,  0.0179968 ,\n",
       "        -0.03531788,  0.12632455, -0.05915659, -0.05977134,  0.07300624,\n",
       "        -0.20184529, -0.06021175, -0.1589274 , -0.0158653 ,  0.14214398,\n",
       "        -0.03911573, -0.11371459],\n",
       "       [-0.06849635,  0.00995546,  0.05816585,  0.03781942, -0.02526987,\n",
       "         0.05539752, -0.17635791,  0.05470857,  0.17366175,  0.01917322,\n",
       "         0.06387094, -0.01698533, -0.00506841, -0.00584116,  0.07189137,\n",
       "        -0.06351052, -0.00188681, -0.08475505, -0.05147968,  0.0179968 ,\n",
       "        -0.03531788,  0.12632455, -0.05915659, -0.05977134,  0.07300624,\n",
       "        -0.20184529, -0.06021175, -0.1589274 , -0.0158653 ,  0.14214398,\n",
       "        -0.03911573, -0.11371459]], dtype=float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=154, shape=(2, 32), dtype=float32, numpy=\n",
       "array([[-0.06849635,  0.00995546,  0.05816585,  0.03781942, -0.02526987,\n",
       "         0.05539752, -0.17635791,  0.05470857,  0.17366175,  0.01917322,\n",
       "         0.06387094, -0.01698533, -0.00506841, -0.00584116,  0.07189137,\n",
       "        -0.06351052, -0.00188681, -0.08475505, -0.05147968,  0.0179968 ,\n",
       "        -0.03531788,  0.12632455, -0.05915659, -0.05977134,  0.07300624,\n",
       "        -0.20184529, -0.06021175, -0.1589274 , -0.0158653 ,  0.14214398,\n",
       "        -0.03911573, -0.11371459],\n",
       "       [-0.06849635,  0.00995546,  0.05816585,  0.03781942, -0.02526987,\n",
       "         0.05539752, -0.17635791,  0.05470857,  0.17366175,  0.01917322,\n",
       "         0.06387094, -0.01698533, -0.00506841, -0.00584116,  0.07189137,\n",
       "        -0.06351052, -0.00188681, -0.08475505, -0.05147968,  0.0179968 ,\n",
       "        -0.03531788,  0.12632455, -0.05915659, -0.05977134,  0.07300624,\n",
       "        -0.20184529, -0.06021175, -0.1589274 , -0.0158653 ,  0.14214398,\n",
       "        -0.03911573, -0.11371459]], dtype=float32)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'linear2_3/Variable:0' shape=(2, 32) dtype=float32, numpy=\n",
       " array([[-0.05283543, -0.0082053 , -0.06088096,  0.07135466, -0.08922812,\n",
       "          0.00388604, -0.00522436,  0.04253149,  0.09767316, -0.06347967,\n",
       "          0.04051628,  0.04747967, -0.03607544,  0.03302233, -0.05037155,\n",
       "         -0.03650972, -0.02882522,  0.01401194,  0.0320702 , -0.0496694 ,\n",
       "         -0.03895427,  0.07002649,  0.01673152, -0.00125052,  0.04609814,\n",
       "         -0.10299792,  0.0235304 ,  0.0149266 , -0.02171146, -0.04270264,\n",
       "          0.07031806, -0.03288479],\n",
       "        [ 0.06132748,  0.02129406, -0.02074615,  0.06057013,  0.0153632 ,\n",
       "         -0.0258691 ,  0.04322994,  0.10100406, -0.04173629,  0.12975429,\n",
       "         -0.03897617, -0.00556219,  0.05746824,  0.0208741 ,  0.0259151 ,\n",
       "         -0.00101555,  0.03396238, -0.12266488,  0.02647762, -0.02423989,\n",
       "          0.01235323,  0.0094605 , -0.04054355,  0.01750218, -0.06966328,\n",
       "         -0.00336607, -0.04003532,  0.0286801 , -0.01431572,  0.00417895,\n",
       "          0.04457145,  0.00347458]], dtype=float32)>,\n",
       " <tf.Variable 'linear2_3/Variable:0' shape=(32,) dtype=float32, numpy=\n",
       " array([ 0.00708512,  0.00151425, -0.01408941, -0.00944196,  0.0253394 ,\n",
       "         0.10641517, -0.03659144, -0.08393039, -0.04712839,  0.08111021,\n",
       "         0.02068461,  0.00110636,  0.04611717, -0.05088538, -0.00841685,\n",
       "         0.09720504,  0.00301134, -0.06866088, -0.00611953, -0.01665061,\n",
       "         0.00930399, -0.00644063,  0.05147564,  0.06973229, -0.00460426,\n",
       "         0.066727  ,  0.0884307 , -0.06239613, -0.0142116 , -0.00821625,\n",
       "        -0.03458769,  0.08545405], dtype=float32)>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer2.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear3(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear3, self).__init__()\n",
    "        self.units = units\n",
    "        print(\"init\")\n",
    "\n",
    "    def build(self ,input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        print(\"build\")\n",
    "\n",
    "    def call(self,defg):\n",
    "        print(\"call\")\n",
    "        return tf.matmul(defg, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n",
      "build\n",
      "call\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=118, shape=(2, 32), dtype=float32, numpy=\n",
       "array([[ 0.05445231, -0.00421354, -0.00246931, -0.01939628,  0.02870491,\n",
       "        -0.08737153,  0.00841595,  0.02016823,  0.22865051, -0.03089071,\n",
       "        -0.05110834, -0.12780133, -0.00230058,  0.06311996,  0.1361354 ,\n",
       "        -0.08656222, -0.03167488, -0.01516738,  0.06812147, -0.02753681,\n",
       "        -0.061635  ,  0.07040352,  0.09777652, -0.10591459, -0.00030224,\n",
       "         0.18461281, -0.00934733, -0.03347958, -0.01383944,  0.06318942,\n",
       "         0.05514618,  0.05762808],\n",
       "       [ 0.05445231, -0.00421354, -0.00246931, -0.01939628,  0.02870491,\n",
       "        -0.08737153,  0.00841595,  0.02016823,  0.22865051, -0.03089071,\n",
       "        -0.05110834, -0.12780133, -0.00230058,  0.06311996,  0.1361354 ,\n",
       "        -0.08656222, -0.03167488, -0.01516738,  0.06812147, -0.02753681,\n",
       "        -0.061635  ,  0.07040352,  0.09777652, -0.10591459, -0.00030224,\n",
       "         0.18461281, -0.00934733, -0.03347958, -0.01383944,  0.06318942,\n",
       "         0.05514618,  0.05762808]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer3 = Linear3(32)\n",
    "linear_layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
